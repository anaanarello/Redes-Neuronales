{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "354434fc",
   "metadata": {},
   "source": [
    "# Autoencoder como detector de fracturas oseas\n",
    "El autoencoder se entrena SOLO con imagenes normales \n",
    "(not fractured) y usa el error de reconstruccion como\n",
    "medida de anomalia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db2b29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# permitir imagenes truncadas\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# reproducibilidad\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "788f52fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 9246\n",
      "Val: 829\n",
      "Test: 506\n"
     ]
    }
   ],
   "source": [
    "# crear dataframes desde carpetas\n",
    "\n",
    "def create_dataframe(root_path):\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "\n",
    "    if not os.path.exists(root_path):\n",
    "        print(f\"Error: La ruta {root_path} no existe.\")\n",
    "        return None\n",
    "\n",
    "    for folder in os.listdir(root_path):\n",
    "        folder_path = os.path.join(root_path, folder)\n",
    "\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    fpath = os.path.join(folder_path, file)\n",
    "                    filepaths.append(fpath)\n",
    "                    labels.append(folder)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'filepath': filepaths,\n",
    "        'label': labels\n",
    "    })\n",
    "    return df\n",
    "\n",
    "\n",
    "train_dir = \"data/Bone_Fracture_Binary_Classification/train\"\n",
    "val_dir   = \"data/Bone_Fracture_Binary_Classification/val\"\n",
    "test_dir  = \"data/Bone_Fracture_Binary_Classification/test\"\n",
    "\n",
    "train_df = create_dataframe(train_dir)\n",
    "val_df   = create_dataframe(val_dir)\n",
    "test_df  = create_dataframe(test_dir)\n",
    "\n",
    "print(\"Train:\", len(train_df))\n",
    "print(\"Val:\", len(val_df))\n",
    "print(\"Test:\", len(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "263b0270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando 9246 imagenes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9246/9246 [01:50<00:00, 83.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagenes corruptas eliminadas: 0\n",
      "Verificando 829 imagenes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 829/829 [00:10<00:00, 78.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagenes corruptas eliminadas: 0\n",
      "Verificando 506 imagenes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [00:11<00:00, 45.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagenes corruptas eliminadas: 0\n",
      "Despues de limpieza:\n",
      "Train: 9246\n",
      "Val: 829\n",
      "Test: 506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#limpieza de imagenes\n",
    "def clean_dataframe(df):\n",
    "    indices_to_drop = []\n",
    "    print(f\"Verificando {len(df)} imagenes...\")\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            img = Image.open(row[\"filepath\"])\n",
    "            img.load()\n",
    "        except:\n",
    "            indices_to_drop.append(idx)\n",
    "\n",
    "    print(f\"Imagenes corruptas eliminadas: {len(indices_to_drop)}\")\n",
    "    return df.drop(indices_to_drop).reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_df = clean_dataframe(train_df)\n",
    "val_df   = clean_dataframe(val_df)\n",
    "test_df  = clean_dataframe(test_df)\n",
    "\n",
    "print(\"Despues de limpieza:\")\n",
    "print(\"Train:\", len(train_df))\n",
    "print(\"Val:\", len(val_df))\n",
    "print(\"Test:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e935371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 289\n",
      "Val batches: 26\n",
      "Test batches: 16\n"
     ]
    }
   ],
   "source": [
    "#dataset y dataloaders\n",
    "\n",
    "class BoneFractureDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # not fractured -> 0 (NORMAL)\n",
    "        # fractured     -> 1 (ANOMALIA)\n",
    "        self.label_map = {\n",
    "            \"not fractured\": 0,\n",
    "            \"fractured\": 1\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['filepath']\n",
    "        label_str = self.dataframe.iloc[idx]['label']\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "        label = self.label_map[label_str]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = BoneFractureDataset(train_df, transform=transform)\n",
    "val_dataset   = BoneFractureDataset(val_df, transform=transform)\n",
    "test_dataset  = BoneFractureDataset(test_df, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Val batches:\", len(val_loader))\n",
    "print(\"Test batches:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7256005b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagenes SOLO NORMALES: 4640\n"
     ]
    }
   ],
   "source": [
    "#filtranos solo clase normal\n",
    "normal_indices_train = [\n",
    "    idx for idx in range(len(train_dataset))\n",
    "    if train_dataset.dataframe.iloc[idx]['label'] == \"not fractured\"\n",
    "]\n",
    "\n",
    "train_normal_dataset = torch.utils.data.Subset(train_dataset, normal_indices_train)\n",
    "train_normal_loader  = DataLoader(train_normal_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"Imagenes SOLO NORMALES:\", len(train_normal_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e69d7890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvAutoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (5): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#autoencoder \n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 3, 2, 1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, 2, 1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, 2, 1, output_padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "\n",
    "ae_model = ConvAutoencoder().to(device)\n",
    "criterion_ae = nn.MSELoss()\n",
    "optimizer_ae = torch.optim.Adam(ae_model.parameters(), lr=1e-3)\n",
    "\n",
    "print(ae_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27a65a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] | AE Train Recon Loss: 0.116365\n",
      "Epoch [2/25] | AE Train Recon Loss: 0.016336\n",
      "Epoch [3/25] | AE Train Recon Loss: 0.012124\n",
      "Epoch [4/25] | AE Train Recon Loss: 0.009908\n",
      "Epoch [5/25] | AE Train Recon Loss: 0.008244\n",
      "Epoch [6/25] | AE Train Recon Loss: 0.006942\n",
      "Epoch [7/25] | AE Train Recon Loss: 0.006058\n",
      "Epoch [8/25] | AE Train Recon Loss: 0.005492\n",
      "Epoch [9/25] | AE Train Recon Loss: 0.005060\n",
      "Epoch [10/25] | AE Train Recon Loss: 0.004724\n",
      "Epoch [11/25] | AE Train Recon Loss: 0.004426\n",
      "Epoch [12/25] | AE Train Recon Loss: 0.004208\n",
      "Epoch [13/25] | AE Train Recon Loss: 0.003977\n",
      "Epoch [14/25] | AE Train Recon Loss: 0.003833\n",
      "Epoch [15/25] | AE Train Recon Loss: 0.003615\n",
      "Epoch [16/25] | AE Train Recon Loss: 0.003484\n",
      "Epoch [17/25] | AE Train Recon Loss: 0.003354\n",
      "Epoch [18/25] | AE Train Recon Loss: 0.003211\n",
      "Epoch [19/25] | AE Train Recon Loss: 0.003101\n",
      "Epoch [20/25] | AE Train Recon Loss: 0.002985\n",
      "Epoch [21/25] | AE Train Recon Loss: 0.002880\n",
      "Epoch [22/25] | AE Train Recon Loss: 0.002811\n",
      "Epoch [23/25] | AE Train Recon Loss: 0.002699\n",
      "Epoch [24/25] | AE Train Recon Loss: 0.002615\n",
      "Epoch [25/25] | AE Train Recon Loss: 0.002550\n"
     ]
    }
   ],
   "source": [
    "#entrenamiento solo con imagenes normales\n",
    "num_epochs_ae = 25\n",
    "train_losses_ae = []\n",
    "\n",
    "for epoch in range(num_epochs_ae):\n",
    "    ae_model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, _ in train_normal_loader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        outputs = ae_model(images)\n",
    "        loss = criterion_ae(outputs, images)\n",
    "\n",
    "        optimizer_ae.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_ae.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_normal_loader)\n",
    "    train_losses_ae.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_ae}] | AE Train Recon Loss: {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47e84966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diana\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Umbral de anomalía (recon error): 0.004583260044455528\n",
      "\n",
      "=== RESULTADOS FINALES DEL AUTOENCODER ===\n",
      "AE Accuracy Train: 59.03%\n",
      "AE Accuracy Val:   68.15%\n",
      "AE Accuracy Test:  55.53%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CHUNK FINAL - AUTOENCODER COMO CLASIFICADOR (ANOMALY DETECTOR)\n",
    "# ============================================================\n",
    "\n",
    "def reconstruction_errors(ae_model, data_loader, device):\n",
    "    ae_model.eval()\n",
    "    all_errors = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            recon = ae_model(images)\n",
    "\n",
    "            # MSE por pixel\n",
    "            loss_per_pixel = F.mse_loss(recon, images, reduction=\"none\")\n",
    "            # promedio por imagen\n",
    "            loss_per_image = loss_per_pixel.view(loss_per_pixel.size(0), -1).mean(dim=1)\n",
    "\n",
    "            all_errors.append(loss_per_image.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    all_errors = torch.cat(all_errors)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    return all_errors, all_labels\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. CALCULAR ERRORES EN VALIDACION\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "val_errors, val_labels = reconstruction_errors(ae_model, val_loader, device)\n",
    "\n",
    "# SOLO errores de la clase NORMAL (0 = not fractured)\n",
    "normal_val_errors = val_errors[val_labels == 0]\n",
    "\n",
    "# UMBRAL = media + 2 * desviacion estandar\n",
    "threshold = normal_val_errors.mean() + 2 * normal_val_errors.std()\n",
    "\n",
    "print(\"Umbral de anomalía (recon error):\", threshold.item())\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. FUNCION PARA CALCULAR ACCURACY CON UMBRAL\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def evaluate_ae_classifier(ae_model, data_loader, device, threshold):\n",
    "    ae_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            recon = ae_model(images)\n",
    "\n",
    "            loss_per_pixel = F.mse_loss(recon, images, reduction=\"none\")\n",
    "            loss_per_image = loss_per_pixel.view(loss_per_pixel.size(0), -1).mean(dim=1)\n",
    "\n",
    "            # prediccion: 0 si <= umbral, 1 si > umbral\n",
    "            preds = (loss_per_image > threshold.to(device)).float()\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * correct / total\n",
    "    return acc\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. ACCURACY FINAL DEL AUTOENCODER\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "ae_train_acc = evaluate_ae_classifier(ae_model, train_loader, device, threshold)\n",
    "ae_val_acc   = evaluate_ae_classifier(ae_model, val_loader, device, threshold)\n",
    "ae_test_acc  = evaluate_ae_classifier(ae_model, test_loader, device, threshold)\n",
    "\n",
    "print(\"\\n=== RESULTADOS FINALES DEL AUTOENCODER ===\")\n",
    "print(f\"AE Accuracy Train: {ae_train_acc:.2f}%\")\n",
    "print(f\"AE Accuracy Val:   {ae_val_acc:.2f}%\")\n",
    "print(f\"AE Accuracy Test:  {ae_test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9964b6",
   "metadata": {},
   "source": [
    "El autoencoder convolucional fue entrenado exclusivamente con imágenes de la clase not fractured como clase normal. Posteriormente, se utilizó el error de reconstrucción para definir un umbral de anomalía, permitiendo clasificar nuevas imágenes como normales o anómalas. El desempeño se evaluó mediante accuracy en los conjuntos de entrenamiento, validación y prueba."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
