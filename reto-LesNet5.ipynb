{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04294b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3a200a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d61a76",
   "metadata": {},
   "source": [
    "## Limpieza de Datos con Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84951c66",
   "metadata": {},
   "source": [
    "### Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ccf3d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 9246 imágenes\n",
      "Val data: 829 imágenes\n",
      "Test data: 506 imágenes\n",
      "                                            filepath      label\n",
      "0  data/Bone_Fracture_Binary_Classification/train...  fractured\n",
      "1  data/Bone_Fracture_Binary_Classification/train...  fractured\n",
      "2  data/Bone_Fracture_Binary_Classification/train...  fractured\n",
      "3  data/Bone_Fracture_Binary_Classification/train...  fractured\n",
      "4  data/Bone_Fracture_Binary_Classification/train...  fractured\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def create_dataframe(root_path):\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "\n",
    "    # Verificamos que la ruta exista\n",
    "    if not os.path.exists(root_path):\n",
    "        print(f\"Error: La ruta {root_path} no existe. Verifica la dirección.\")\n",
    "        return None\n",
    "\n",
    "    # Recorrer las carpetas (clases)\n",
    "    for folder in os.listdir(root_path):\n",
    "        folder_path = os.path.join(root_path, folder)\n",
    "        \n",
    "        # Solo entramos si es una carpeta (ej: 'fractured', 'not_fractured')\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file in os.listdir(folder_path):\n",
    "                # Filtrar solo imágenes\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    fpath = os.path.join(folder_path, file)\n",
    "                    filepaths.append(fpath)\n",
    "                    labels.append(folder)\n",
    "    \n",
    "    # Crear y retornar el DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'filepath': filepaths,\n",
    "        'label': labels\n",
    "    })\n",
    "    return df\n",
    "\n",
    "# Rutas a los datasets\n",
    "train_dir = 'data/Bone_Fracture_Binary_Classification/train' \n",
    "val_dir = 'data/Bone_Fracture_Binary_Classification/val' \n",
    "test_dir = 'data/Bone_Fracture_Binary_Classification/test'  \n",
    "\n",
    "# Crear los DataFrames\n",
    "train_df = create_dataframe(train_dir)\n",
    "val_df = create_dataframe(val_dir)\n",
    "test_df = create_dataframe(test_dir)\n",
    "\n",
    "# Verificación rápida\n",
    "print(f\"Train data: {len(train_df)} imágenes\")\n",
    "print(f\"Val data: {len(val_df)} imágenes\")\n",
    "print(f\"Test data: {len(test_df)} imágenes\")\n",
    "print(train_df.head()) # Para ver que las columnas 'filepath' y 'label' existen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "829da752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la clase del Dataset\n",
    "class BoneFractureDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame con columnas 'filepath' y 'label'.\n",
    "            transform (callable, optional): Transformaciones (resize, normalize, etc).\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Mapeamos etiquetas de texto a números (ej: 'fractured': 1, 'normal': 0)\n",
    "        # Ajusta esto según los nombres reales de tus carpetas\n",
    "        self.label_map = {label: idx for idx, label in enumerate(dataframe['label'].unique())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Obtener la ruta de la imagen\n",
    "        img_path = self.dataframe.iloc[idx]['filepath']\n",
    "        \n",
    "        # 2. Cargar imagen (convertir a gris para LeNet)\n",
    "        image = Image.open(img_path).convert('L') \n",
    "        \n",
    "        # 3. Obtener etiqueta numérica\n",
    "        label_str = self.dataframe.iloc[idx]['label']\n",
    "        label = self.label_map[label_str]\n",
    "        \n",
    "        # 4. Aplicar transformaciones\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # Retornamos la imagen y su etiqueta (como tensor flotante para la loss function)\n",
    "        return image, torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d8f884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir transformaciones\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)), # Tamaño clásico LeNet\n",
    "    transforms.ToTensor(),       # Convierte a Tensor y escala a [0, 1]\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]) # Normalización estándar (-1 a 1)\n",
    "])\n",
    "\n",
    "# Instanciar el Dataset usando el DataFrame 'val_df' que creamos antes\n",
    "# (Asumiendo que hiciste lo mismo para un 'train_df')\n",
    "train_dataset = BoneFractureDataset(train_df, transform=transform)\n",
    "val_dataset = BoneFractureDataset(val_df, transform=transform)\n",
    "test_dataset = BoneFractureDataset(test_df, transform=transform)\n",
    "\n",
    "# Crear los DataLoaders (esto es lo que itera sobre los datos)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f77f8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando 9246 imágenes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 4884/9246 [00:07<00:04, 905.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/train/not fractured/IMG0004143.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 5577/9246 [00:07<00:03, 1117.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/train/not fractured/IMG0004347.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 6006/9246 [00:08<00:03, 865.37it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/train/not fractured/IMG0004134.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6180/9246 [00:08<00:04, 741.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/train/not fractured/IMG0004308.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 8551/9246 [00:11<00:00, 1089.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/train/not fractured/IMG0004148.jpg\n",
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/train/not fractured/IMG0004149.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9246/9246 [00:11<00:00, 779.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando 506 imágenes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 273/506 [00:00<00:00, 327.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/test/not fractured/IMG0004143.jpg\n",
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/test/not fractured/IMG0004347.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 308/506 [00:00<00:00, 309.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/test/not fractured/IMG0004134.jpg\n",
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/test/not fractured/IMG0004308.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 491/506 [00:01<00:00, 211.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/test/not fractured/IMG0004148.jpg\n",
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/test/not fractured/IMG0004149.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [00:02<00:00, 245.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando 829 imágenes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 413/829 [00:01<00:01, 277.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/val/not fractured/IMG0004143.jpg\n",
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/val/not fractured/IMG0004347.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 491/829 [00:01<00:01, 258.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/val/not fractured/IMG0004134.jpg\n",
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/val/not fractured/IMG0004308.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 803/829 [00:02<00:00, 359.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/val/not fractured/IMG0004148.jpg\n",
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/val/not fractured/IMG0004149.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 829/829 [00:03<00:00, 268.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limpieza completada. Puedes entrenar ahora.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm # Barra de progreso\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    indices_to_drop = []\n",
    "    print(f\"Verificando {len(df)} imágenes...\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        try:\n",
    "            # Intentamos abrir la imagen completamente\n",
    "            img = Image.open(row['filepath'])\n",
    "            img.load() # Forzamos la lectura de los datos\n",
    "        except OSError:\n",
    "            print(f\"Imagen corrupta encontrada y eliminada: {row['filepath']}\")\n",
    "            indices_to_drop.append(idx)\n",
    "        except Exception as e:\n",
    "            print(f\"Error en imagen {row['filepath']}: {e}\")\n",
    "            indices_to_drop.append(idx)\n",
    "            \n",
    "    # Eliminamos las filas malas\n",
    "    return df.drop(indices_to_drop)\n",
    "\n",
    "# Limpiamos el dataframe de entrenamiento\n",
    "train_df = clean_dataframe(train_df)\n",
    "test_df = clean_dataframe(test_df)\n",
    "val_df = clean_dataframe(val_df)\n",
    "\n",
    "# IMPORTANTE: Recrear el Dataset y el DataLoader con el dataframe limpio\n",
    "train_dataset = BoneFractureDataset(train_df, transform=transform)\n",
    "val_dataset = BoneFractureDataset(val_df, transform=transform)\n",
    "test_dataset = BoneFractureDataset(test_df, transform=transform)\n",
    "\n",
    "# Crear los DataLoaders (esto es lo que itera sobre los datos)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"Limpieza completada. Puedes entrenar ahora.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdf2831",
   "metadata": {},
   "source": [
    "## LeNet5: Una Arquitectura Clásica de Red Neuronal Convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26167ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3929bcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo LeNet5 actualizado para clasificación binaria (1 salida).\n"
     ]
    }
   ],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        # C1: 1 canal de entrada (Escala de grises) -> 6 canales de salida\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0)\n",
    "        # S2: Pooling\n",
    "        self.avgpool1 = nn.AvgPool2d(2, 2)\n",
    "        # C3: 6 -> 16\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        # S4: Pooling\n",
    "        self.avgpool2 = nn.AvgPool2d(2, 2)\n",
    "        \n",
    "        # C5: Flatten. \n",
    "        # Si la imagen entra de 32x32, después de las convulsiones queda de 5x5.\n",
    "        # 16 canales * 5 * 5 = 400 entradas\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "\n",
    "        # FC6\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "\n",
    "        # Ahora: \n",
    "        self.fc3 = nn.Linear(84, 1)  # <-- Solo 1 neurona para Binary Classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.conv1(x))\n",
    "        x = self.avgpool1(x)\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        x = self.avgpool2(x)\n",
    "        \n",
    "        # Aplanamos (Flatten)\n",
    "        x = x.view(x.size(0), -1) \n",
    "\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "\n",
    "        x = self.fc3(x)  # Retorna el logit (sin sigmoide)\n",
    "\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LeNet5().to(device)\n",
    "\n",
    "# Como recreamos el modelo, debemos recrear el optimizador (para que coja los nuevos parámetros)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Modelo LeNet5 actualizado para clasificación binaria (1 salida).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c2346a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] | Train Loss: 0.5765 | Val Loss: 0.5353 | Val Acc: 74.73%\n",
      "Epoch [2/25] | Train Loss: 0.4711 | Val Loss: 0.4643 | Val Acc: 79.59%\n",
      "Epoch [3/25] | Train Loss: 0.3939 | Val Loss: 0.4425 | Val Acc: 80.07%\n",
      "Epoch [4/25] | Train Loss: 0.2727 | Val Loss: 0.3382 | Val Acc: 87.12%\n",
      "Epoch [5/25] | Train Loss: 0.1481 | Val Loss: 0.2754 | Val Acc: 89.91%\n",
      "Epoch [6/25] | Train Loss: 0.0836 | Val Loss: 0.1846 | Val Acc: 90.77%\n",
      "Epoch [7/25] | Train Loss: 0.0496 | Val Loss: 0.1522 | Val Acc: 93.92%\n",
      "Epoch [8/25] | Train Loss: 0.0303 | Val Loss: 0.1543 | Val Acc: 92.83%\n",
      "Epoch [9/25] | Train Loss: 0.0204 | Val Loss: 0.1606 | Val Acc: 94.65%\n",
      "Epoch [10/25] | Train Loss: 0.0190 | Val Loss: 0.1670 | Val Acc: 94.78%\n",
      "Epoch [11/25] | Train Loss: 0.0257 | Val Loss: 0.1192 | Val Acc: 96.11%\n",
      "Epoch [12/25] | Train Loss: 0.0095 | Val Loss: 0.0936 | Val Acc: 97.81%\n",
      "Epoch [13/25] | Train Loss: 0.0059 | Val Loss: 0.0965 | Val Acc: 97.21%\n",
      "Epoch [14/25] | Train Loss: 0.0043 | Val Loss: 0.1072 | Val Acc: 97.21%\n",
      "Epoch [15/25] | Train Loss: 0.0269 | Val Loss: 0.1182 | Val Acc: 96.48%\n",
      "Epoch [16/25] | Train Loss: 0.0071 | Val Loss: 0.0991 | Val Acc: 97.33%\n",
      "Epoch [17/25] | Train Loss: 0.0046 | Val Loss: 0.1221 | Val Acc: 95.99%\n",
      "Epoch [18/25] | Train Loss: 0.0025 | Val Loss: 0.1064 | Val Acc: 96.84%\n",
      "Epoch [19/25] | Train Loss: 0.0030 | Val Loss: 0.0962 | Val Acc: 97.69%\n",
      "Epoch [20/25] | Train Loss: 0.0032 | Val Loss: 0.1093 | Val Acc: 97.69%\n",
      "Epoch [21/25] | Train Loss: 0.0466 | Val Loss: 0.1066 | Val Acc: 97.57%\n",
      "Epoch [22/25] | Train Loss: 0.0021 | Val Loss: 0.1087 | Val Acc: 97.69%\n",
      "Epoch [23/25] | Train Loss: 0.0008 | Val Loss: 0.1036 | Val Acc: 97.69%\n",
      "Epoch [24/25] | Train Loss: 0.0005 | Val Loss: 0.0964 | Val Acc: 97.93%\n",
      "Epoch [25/25] | Train Loss: 0.0004 | Val Loss: 0.0954 | Val Acc: 97.81%\n",
      "\n",
      "--- Entrenamiento terminado. Evaluando en Test Set ---\n",
      "RESULTADO FINAL (Test Accuracy): 99.20%\n"
     ]
    }
   ],
   "source": [
    "# Hiperparámetros\n",
    "num_epochs = 25\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Definir la función de pérdida y el optimizador\n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# BUCLE PRINCIPAL (Entrenamiento y Validación) \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # --- FASE DE ENTRENAMIENTO ---\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # BCEWithLogitsLoss requiere float en targets, no long/int\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # FASE DE VALIDACIÓN \n",
    "    model.eval() \n",
    "    val_loss = 0.0 \n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            # También convertimos labels aquí para calcular la loss de validación y comparar\n",
    "            labels = labels.to(device).float().unsqueeze(1) \n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Calculamos también la loss de validación \n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Predicción: Sigmoid > 0.5 equivale a Logits > 0\n",
    "            predicted = (outputs > 0).float() \n",
    "            \n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        avg_val_loss = val_loss / len(val_loader) # Promedio loss validación\n",
    "    \n",
    "    # Imprimimos reporte más completo\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] | '\n",
    "          f'Train Loss: {avg_train_loss:.4f} | '\n",
    "          f'Val Loss: {avg_val_loss:.4f} | ' # <--- Dato útil para ver overfitting\n",
    "          f'Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "\n",
    "#  EVALUACIÓN FINAL\n",
    "print(\"\\n--- Entrenamiento terminado. Evaluando en Test Set ---\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        predicted = (outputs > 0).float()\n",
    "        \n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "    print(f'RESULTADO FINAL (Test Accuracy): {test_accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
