{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04294b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d61a76",
   "metadata": {},
   "source": [
    "## Limpieza de Datos con Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84951c66",
   "metadata": {},
   "source": [
    "### Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ccf3d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 9246 imágenes\n",
      "Val data: 829 imágenes\n",
      "Test data: 506 imágenes\n",
      "                                            filepath      label\n",
      "0  data/Bone_Fracture_Binary_Classification/train...  fractured\n",
      "1  data/Bone_Fracture_Binary_Classification/train...  fractured\n",
      "2  data/Bone_Fracture_Binary_Classification/train...  fractured\n",
      "3  data/Bone_Fracture_Binary_Classification/train...  fractured\n",
      "4  data/Bone_Fracture_Binary_Classification/train...  fractured\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def create_dataframe(root_path):\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "\n",
    "    # Verificamos que la ruta exista\n",
    "    if not os.path.exists(root_path):\n",
    "        print(f\"Error: La ruta {root_path} no existe. Verifica la dirección.\")\n",
    "        return None\n",
    "\n",
    "    # Recorrer las carpetas (clases)\n",
    "    for folder in os.listdir(root_path):\n",
    "        folder_path = os.path.join(root_path, folder)\n",
    "        \n",
    "        # Solo entramos si es una carpeta (ej: 'fractured', 'not_fractured')\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file in os.listdir(folder_path):\n",
    "                # Filtrar solo imágenes\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    fpath = os.path.join(folder_path, file)\n",
    "                    filepaths.append(fpath)\n",
    "                    labels.append(folder)\n",
    "    \n",
    "    # Crear y retornar el DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'filepath': filepaths,\n",
    "        'label': labels\n",
    "    })\n",
    "    return df\n",
    "\n",
    "# Rutas a los datasets\n",
    "train_dir = 'data/Bone_Fracture_Binary_Classification/train' \n",
    "val_dir = 'data/Bone_Fracture_Binary_Classification/val' \n",
    "test_dir = 'data/Bone_Fracture_Binary_Classification/test'  \n",
    "\n",
    "# Crear los DataFrames\n",
    "train_df = create_dataframe(train_dir)\n",
    "val_df = create_dataframe(val_dir)\n",
    "test_df = create_dataframe(test_dir)\n",
    "\n",
    "# Verificación rápida\n",
    "print(f\"Train data: {len(train_df)} imágenes\")\n",
    "print(f\"Val data: {len(val_df)} imágenes\")\n",
    "print(f\"Test data: {len(test_df)} imágenes\")\n",
    "print(train_df.head()) # Para ver que las columnas 'filepath' y 'label' existen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "829da752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la clase del Dataset\n",
    "class BoneFractureDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame con columnas 'filepath' y 'label'.\n",
    "            transform (callable, optional): Transformaciones (resize, normalize, etc).\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Mapeamos etiquetas de texto a números (ej: 'fractured': 1, 'normal': 0)\n",
    "        # Ajusta esto según los nombres reales de tus carpetas\n",
    "        self.label_map = {label: idx for idx, label in enumerate(dataframe['label'].unique())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Obtener la ruta de la imagen\n",
    "        img_path = self.dataframe.iloc[idx]['filepath']\n",
    "        \n",
    "        # 2. Cargar imagen (convertir a gris para LeNet)\n",
    "        image = Image.open(img_path).convert('L') \n",
    "        \n",
    "        # 3. Obtener etiqueta numérica\n",
    "        label_str = self.dataframe.iloc[idx]['label']\n",
    "        label = self.label_map[label_str]\n",
    "        \n",
    "        # 4. Aplicar transformaciones\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # Retornamos la imagen y su etiqueta (como tensor flotante para la loss function)\n",
    "        return image, torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d8f884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir transformaciones\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)), # Tamaño clásico LeNet\n",
    "    transforms.ToTensor(),       # Convierte a Tensor y escala a [0, 1]\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]) # Normalización estándar (-1 a 1)\n",
    "])\n",
    "\n",
    "# Instanciar el Dataset usando el DataFrame 'val_df' que creamos antes\n",
    "# (Asumiendo que hiciste lo mismo para un 'train_df')\n",
    "train_dataset = BoneFractureDataset(train_df, transform=transform)\n",
    "val_dataset = BoneFractureDataset(val_df, transform=transform)\n",
    "test_dataset = BoneFractureDataset(test_df, transform=transform)\n",
    "\n",
    "# Crear los DataLoaders (esto es lo que itera sobre los datos)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f77f8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando 9246 imágenes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 4884/9246 [00:06<00:04, 975.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/train/not fractured/IMG0004143.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 5519/9246 [00:07<00:03, 1147.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/train/not fractured/IMG0004347.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6026/9246 [00:07<00:03, 981.24it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/train/not fractured/IMG0004134.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6232/9246 [00:07<00:03, 877.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/train/not fractured/IMG0004308.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 8576/9246 [00:10<00:00, 1049.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/train/not fractured/IMG0004148.jpg\n",
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/train/not fractured/IMG0004149.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9246/9246 [00:10<00:00, 846.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando 506 imágenes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 274/506 [00:00<00:00, 369.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/test/not fractured/IMG0004143.jpg\n",
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/test/not fractured/IMG0004347.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 315/506 [00:00<00:00, 308.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/test/not fractured/IMG0004134.jpg\n",
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/test/not fractured/IMG0004308.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 473/506 [00:01<00:00, 235.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/test/not fractured/IMG0004148.jpg\n",
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/test/not fractured/IMG0004149.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [00:01<00:00, 267.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando 829 imágenes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 442/829 [00:01<00:01, 328.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/val/not fractured/IMG0004143.jpg\n",
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/val/not fractured/IMG0004347.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 513/829 [00:01<00:01, 268.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/val/not fractured/IMG0004134.jpg\n",
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/val/not fractured/IMG0004308.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 814/829 [00:02<00:00, 366.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/val/not fractured/IMG0004148.jpg\n",
      "Imagen corrupta encontrada y eliminada: data/Bone_Fracture_Binary_Classification/val/not fractured/IMG0004149.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 829/829 [00:02<00:00, 293.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limpieza completada. Puedes entrenar ahora.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm # Barra de progreso\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    indices_to_drop = []\n",
    "    print(f\"Verificando {len(df)} imágenes...\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        try:\n",
    "            # Intentamos abrir la imagen completamente\n",
    "            img = Image.open(row['filepath'])\n",
    "            img.load() # Forzamos la lectura de los datos\n",
    "        except OSError:\n",
    "            print(f\"Imagen corrupta encontrada y eliminada: {row['filepath']}\")\n",
    "            indices_to_drop.append(idx)\n",
    "        except Exception as e:\n",
    "            print(f\"Error en imagen {row['filepath']}: {e}\")\n",
    "            indices_to_drop.append(idx)\n",
    "            \n",
    "    # Eliminamos las filas malas\n",
    "    return df.drop(indices_to_drop)\n",
    "\n",
    "# Limpiamos el dataframe de entrenamiento\n",
    "train_df = clean_dataframe(train_df)\n",
    "test_df = clean_dataframe(test_df)\n",
    "val_df = clean_dataframe(val_df)\n",
    "\n",
    "# IMPORTANTE: Recrear el Dataset y el DataLoader con el dataframe limpio\n",
    "train_dataset = BoneFractureDataset(train_df, transform=transform)\n",
    "val_dataset = BoneFractureDataset(val_df, transform=transform)\n",
    "test_dataset = BoneFractureDataset(test_df, transform=transform)\n",
    "\n",
    "# Crear los DataLoaders (esto es lo que itera sobre los datos)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"Limpieza completada. Puedes entrenar ahora.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdf2831",
   "metadata": {},
   "source": [
    "## LeNet5: Una Arquitectura Clásica de Red Neuronal Convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26167ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from search_space_students import HPOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3929bcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo LeNet5 actualizado para clasificación binaria (1 salida).\n"
     ]
    }
   ],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        # C1: 1 canal de entrada (Escala de grises) -> 6 canales de salida\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0)\n",
    "        # S2: Pooling\n",
    "        self.avgpool1 = nn.AvgPool2d(2, 2)\n",
    "        # C3: 6 -> 16\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        # S4: Pooling\n",
    "        self.avgpool2 = nn.AvgPool2d(2, 2)\n",
    "        \n",
    "        # C5: Flatten. \n",
    "        # Si la imagen entra de 32x32, después de las convulsiones queda de 5x5.\n",
    "        # 16 canales * 5 * 5 = 400 entradas\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "\n",
    "        # FC6\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "\n",
    "        # Output: CAMBIO CRÍTICO AQUÍ\n",
    "        # Antes: self.fc3 = nn.Linear(84, 10)  <-- Esto era para 10 clases\n",
    "        # Ahora: \n",
    "        self.fc3 = nn.Linear(84, 1)  # <-- Solo 1 neurona para Binary Classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.conv1(x))\n",
    "        x = self.avgpool1(x)\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        x = self.avgpool2(x)\n",
    "        \n",
    "        # Aplanamos (Flatten)\n",
    "        x = x.view(x.size(0), -1) \n",
    "\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "\n",
    "        x = self.fc3(x)  # Retorna el logit (sin sigmoide, porque usas BCEWithLogitsLoss)\n",
    "\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LeNet5().to(device)\n",
    "\n",
    "# Como recreamos el modelo, debemos recrear el optimizador (para que coja los nuevos parámetros)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Modelo LeNet5 actualizado para clasificación binaria (1 salida).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c2346a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] | Train Loss: 0.6104 | Val Loss: 0.5805 | Val Acc: 72.05%\n",
      "Epoch [2/25] | Train Loss: 0.5381 | Val Loss: 0.5598 | Val Acc: 71.93%\n",
      "Epoch [3/25] | Train Loss: 0.4874 | Val Loss: 0.5273 | Val Acc: 76.43%\n",
      "Epoch [4/25] | Train Loss: 0.3681 | Val Loss: 0.4558 | Val Acc: 80.32%\n",
      "Epoch [5/25] | Train Loss: 0.2465 | Val Loss: 0.3272 | Val Acc: 86.63%\n",
      "Epoch [6/25] | Train Loss: 0.1277 | Val Loss: 0.2347 | Val Acc: 92.22%\n",
      "Epoch [7/25] | Train Loss: 0.0645 | Val Loss: 0.2021 | Val Acc: 92.71%\n",
      "Epoch [8/25] | Train Loss: 0.0397 | Val Loss: 0.1889 | Val Acc: 93.56%\n",
      "Epoch [9/25] | Train Loss: 0.0311 | Val Loss: 0.1472 | Val Acc: 94.41%\n",
      "Epoch [10/25] | Train Loss: 0.0199 | Val Loss: 0.1092 | Val Acc: 96.35%\n",
      "Epoch [11/25] | Train Loss: 0.0184 | Val Loss: 0.0816 | Val Acc: 97.08%\n",
      "Epoch [12/25] | Train Loss: 0.0136 | Val Loss: 0.1574 | Val Acc: 95.14%\n",
      "Epoch [13/25] | Train Loss: 0.0093 | Val Loss: 0.0878 | Val Acc: 97.69%\n",
      "Epoch [14/25] | Train Loss: 0.0059 | Val Loss: 0.1024 | Val Acc: 97.81%\n",
      "Epoch [15/25] | Train Loss: 0.0228 | Val Loss: 0.1134 | Val Acc: 95.99%\n",
      "Epoch [16/25] | Train Loss: 0.0058 | Val Loss: 0.0873 | Val Acc: 97.81%\n",
      "Epoch [17/25] | Train Loss: 0.0015 | Val Loss: 0.1059 | Val Acc: 97.21%\n",
      "Epoch [18/25] | Train Loss: 0.0012 | Val Loss: 0.0860 | Val Acc: 97.81%\n",
      "Epoch [19/25] | Train Loss: 0.0006 | Val Loss: 0.0846 | Val Acc: 97.81%\n",
      "Epoch [20/25] | Train Loss: 0.0004 | Val Loss: 0.0952 | Val Acc: 97.93%\n",
      "Epoch [21/25] | Train Loss: 0.0003 | Val Loss: 0.0952 | Val Acc: 97.81%\n",
      "Epoch [22/25] | Train Loss: 0.0002 | Val Loss: 0.0930 | Val Acc: 97.93%\n",
      "Epoch [23/25] | Train Loss: 0.0002 | Val Loss: 0.0955 | Val Acc: 97.93%\n",
      "Epoch [24/25] | Train Loss: 0.0001 | Val Loss: 0.0968 | Val Acc: 97.93%\n",
      "Epoch [25/25] | Train Loss: 0.0001 | Val Loss: 0.0992 | Val Acc: 97.93%\n",
      "\n",
      "--- Entrenamiento terminado. Evaluando en Test Set ---\n",
      "RESULTADO FINAL (Test Accuracy): 99.20%\n"
     ]
    }
   ],
   "source": [
    "# Hiperparámetros\n",
    "num_epochs = 25\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Asumimos que 'model', 'train_loader', 'val_loader', 'test_loader' y 'device' ya están definidos arriba\n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- 1. BUCLE PRINCIPAL (Entrenamiento y Validación) ---\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # --- FASE DE ENTRENAMIENTO ---\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # CORRECCIÓN CRÍTICA: Convertir labels a float\n",
    "        # BCEWithLogitsLoss requiere float en targets, no long/int\n",
    "        labels = labels.to(device).float().unsqueeze(1) # <--- Agregado .float()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # --- FASE DE VALIDACIÓN ---\n",
    "    model.eval() \n",
    "    val_loss = 0.0 \n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            # También convertimos labels aquí para calcular la loss de validación y comparar\n",
    "            labels = labels.to(device).float().unsqueeze(1) # <--- Agregado .float()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Calculamos también la loss de validación (buena práctica)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Predicción: Sigmoid > 0.5 equivale a Logits > 0\n",
    "            predicted = (outputs > 0).float() \n",
    "            \n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        avg_val_loss = val_loss / len(val_loader) # Promedio loss validación\n",
    "    \n",
    "    # Imprimimos reporte más completo\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] | '\n",
    "          f'Train Loss: {avg_train_loss:.4f} | '\n",
    "          f'Val Loss: {avg_val_loss:.4f} | ' # <--- Dato útil para ver overfitting\n",
    "          f'Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "\n",
    "# --- 2. EVALUACIÓN FINAL (Test Set) ---\n",
    "print(\"\\n--- Entrenamiento terminado. Evaluando en Test Set ---\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).float().unsqueeze(1) # <--- Agregado .float()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        predicted = (outputs > 0).float()\n",
    "        \n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "    print(f'RESULTADO FINAL (Test Accuracy): {test_accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
